#LyX file created by tex2lyx 2.0.5.1
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\paperfontsize default
\spacing single
\use_hyperref 0
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 0
\use_mathdots 0
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section

Introduction
\end_layout

\begin_layout Standard

Large-volume spectroscopic surveys of the Universe (CITE SDSS, WiggleZ, BOSS) are revolutionizing our understanding of cosmology and structure formation. Based on these successes, a new generation of surveys (CITE DESI, Euclid, WFIRST) is being planned that will improve our constraints by an order of magnitude (or more). This unprecedented improvement in statistical precision places stringent demands on the theoretical modeling and analysis techniques; simulations will play an essential in meeting these requirements.
\end_layout

\begin_layout Standard

One of the challenges for simulations are the varied roles they play, and the different requirements these impose on the simulations. At one extreme, simulations are necessary for generating sample covariances for the data and measurements. This typically requires very large volumes to simulate entire surveys thousands of times, but have lower accuracy requirements. Motivated by these considerations, a number of recent studies have investigated methods designed to produce mock catalogs with reduced accuracy, but much higher throughput compared to the full N-body simulations
\begin_inset space ~

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
after ""
key "2002ApJ...564....8M,2002MNRAS.331..587M,2008MNRAS.391..435F, 2013AN....334..691R,2013arXiv1312.2013C,2013JCAP...06..036T,2014MNRAS.437.2594W, 2013MNRAS.433.2389M,2001A&A...367...18H,2009ApJ...701..945S,2014MNRAS.439L..21K,2014arXiv1409.1124C"

\end_inset

. An open question still is the effect of changing the input cosmology used to generate the covariance matrix on cosmological inferences, and how best to implement such variations. More recently, the impact of super-survey modes (modes outside the survey volume) on inferred errors has been shown to be potentially larger than previously appreciated (CITES??) and is an area of active study.
\end_layout

\begin_layout Standard

At the other extreme, simulations are crucial for calibrating the theoretical models used to fit the data. Examples here are quantifying shifts in the baryon acoustic oscillation distance scale due to nonlinear evolution and galaxy bias (CITES), or templates used to fit the full shape of the correlation function. For such applications, one ideally requires high fidelity simulations. The volume requirements are significantly reduced from that for covariance matrices, but still need to much larger than survey volumes to keep systematic errors below statistical errors.
\end_layout

\begin_layout Standard

An intermediate application are the generation of mock catalogs that capture the observational characteristics of surveys (eg. geometry, selection effects). The importance of these cannot be underestimated, since the effects of many observational systematics can only be quantitatively estimated by simulating them. These issues will get progressively more important for the next generations of surveys which will move away from highly complete and pure samples that have mostly been used for cosmological studies to date.
\end_layout

\begin_layout Standard

The simplest way to generate approximate density fields is to use analytic approximations such as Lagrangian perturbation theory followed by prescriptions to put in halos in a way that better matches results from N-body simulations
\begin_inset space ~

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
after ""
key "2013MNRAS.428.1036M,2014arXiv1401.4171M"

\end_inset

, or simply to run lower resolution N-body codes with a small number of time-steps
\begin_inset space ~

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
after ""
key "2013JCAP...06..036T"

\end_inset

, or a combination of the two approaches
\begin_inset space ~

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
after ""
key "2014MNRAS.437.2594W"

\end_inset

. These methods are successful in capturing the large-scale density field but lose information at small scales. Because of their speed, they can be used to produce large numbers of simulations required to build sample covariance matrices, at error levels ranging from 
\begin_inset Formula $5-10\%$
\end_inset

 (depending on the quantities being predicted). It is difficult to estimate, however, what the loss of accuracy implies for tests of systematic errors, which may need to be modeled at the 
\begin_inset Formula $\sim1\%$
\end_inset

 level.
\end_layout

\begin_layout Standard

The approach we take here is to reduce the small-scale accuracy of a high-resolution N-body code by coarsening its temporal resolution. In the present case, the time-stepping consists of two components, (i) a long time step for solving for evolution under the long-range particle-mesh (PM) force, and (ii) a set of underlying sub-cycled time steps for a short-range particle-particle interaction, computed either via a tree-based algorithm, or by direct particle-particle force evaluations. The idea is to reduce the number of both types of time steps while preserving enough accuracy to correctly describe the large scale distribution of galaxies, as modeled by a halo occupation distribution (HOD) approach. Our first goal in this paper is, therefore, to quantitatively understand the impact of the temporal resolution on the halo density field and how best to accurately reproduce the details of the halo density field on large scales, sacrificing small scale structure information. This allows to generate a suite of large volume simulations, spanning a range of cosmologies. This paper presents the details of these simulations and outlines future applications.
\end_layout

\begin_layout Standard

This paper is organized as follows. Sec.
\begin_inset space ~

\end_inset

2 briefly describes the Hardware/Hybrid Accelerated Cosmology Code (HACC) N-body framework we use to generate our simulations, focusing on the flexibility in the time-stepping that we exploit here. Sec.
\begin_inset space ~

\end_inset

3 presents a sequence of convergence tests where we evaluate the effects of time-stepping on the halo density field. Sec.
\begin_inset space ~

\end_inset

4 discusses interpolating between saved time steps, necessary for constructing light-cone outputs. Sec.
\begin_inset space ~

\end_inset

5 presents an example application of these simulations : generating mock catalogs that match the BOSS galaxy sample. (NOTE : Can we put in another simple application?) We conclude in Sec.
\begin_inset space ~

\end_inset

6 by outlining possible future directions.
\end_layout

\begin_layout Standard

All simulations and calculations in this paper assume a 
\begin_inset Formula $\Lambda$
\end_inset

CDM cosmology with 
\begin_inset Formula $\Omega_{m}=0.2648$
\end_inset

, 
\begin_inset Formula $\Omega_{\Lambda}=0.7352$
\end_inset

, 
\begin_inset Formula $\Omega_{b}h^{2}=0.02258$
\end_inset

, 
\begin_inset Formula $n_{s}=0.963$
\end_inset

, 
\begin_inset Formula $\sigma_{8}=0.8$
\end_inset

 and 
\begin_inset Formula $h=0.71$
\end_inset

.
\end_layout

\end_body
\end_document
