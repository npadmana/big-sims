%% LyX 2.0.5.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[usenatbib]{article}
\usepackage[latin9]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose}
\usepackage{color}
\usepackage{graphicx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
%% A simple dot to overcome graphicx limitations
\newcommand{\lyxdot}{.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\usepackage{jcappub}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% A simple dot to overcome graphicx limitations
%Make my life significantly easier
\usepackage{lineno}\global\long\def\bd{{\bm{\delta}}}
\usepackage{astrobib_mnras2e}
\linenumbers

\makeatother

\begin{document}

\title{Generating Mock Catalogs for the Baryon Oscillation Spectroscopic
Survey: An Approximate N-Body approach}


\author{Tomomi Sunayama, Nikhil Padmanabhan, Katrin Heitmann, Salman Habib,
Steve Rangel}


\abstract{We introduce and test an approximate scheme for generating mock catalogs
for large-scale structure measurements in galaxy surveys, specializing
in this work to the Baryon Oscillation Spectroscopic Survey. things
to add later...A brief description of the approximation scheme, tests
and the accuracy we reach, and some comments about the timings of
the tests and the BOSS samples.}

\maketitle

\section{Introduction}

Recently, the necessity and the demand for the large number of large
N-body simulations have been increased in astronomy due to the precision
required for measurements to understand cosmic acceleration, like
the Baryon Oscillation Spectroscopic Survey (BOSS) and the proposed
Mid Scale-Dark Energy Spectroscopic Instrument (MS-DESI). As the area
covered by those current and future spectroscopic galaxy surveys get
larger, galaxy mocks necessary to the cosmological analysis also have
to be generated from N-body simulations with the corresponding large
volume, which is computationally expensive. Many studies have been
done recently on the implementation of large N-body simulations \cite{2002ApJ...564....8M,2002MNRAS.331..587M,2008MNRAS.391..435F,2013AN....334..691R,2013arXiv1312.2013C,2013JCAP...06..036T,2014MNRAS.437.2594W,2013MNRAS.433.2389M,2001A&A...367...18H,2009ApJ...701..945S}.
Besides the large volume required for the N-body simulations, we need
many realizations to achieve the precision required for those measurements.

There are several reasons for why we need large number of N-body simulations.
One is to understand and to calibrate systematics caused by non-linear
gravitational evolution and galaxy formation. Since those systematics
are cosmology dependent, ideally we want to generate galaxy mocks
from N-body simulations with various cosmologies. Another reason is
to reduce noise for a covariance matrix estimation. Even when 50 realizations
are used, the directly estimated covariance matrix is still noisy.
Since it is unrealistic to run N-body simulations enough number of
times to obtain a smooth directly estimated covariance matrix, there
have been several efforts to obtain a smooth covariance matrix mainly
in two directions. One approach is to estimate a covariance matrix
from theory or by fitting to a modified form of the Gaussian covariance
matrix \cite{2005MNRAS.362..505C,2006MNRAS.371.1188H,2008MNRAS.389..766P,2009MNRAS.396...19N,2010MNRAS.401.2148P,2011MNRAS.415.2876B,2012JCAP...04..019D,2012MNRAS.427.2146X}.
There are many different methods to estimate, but a peculiarity common
to all the methods included in this approach requires some assumptions
to compute the covariance matrix. Those assumptions often prevent
those covariance matrices from properly accounting the effects due
to non-linear gravitational evolution such as non-gaussianities and
non-localities.

Other approach is generating enough number of galaxy mocks by using
2nd Order Perturbation Theory (2LPT) and computing a covariance matrix
estimation directly \cite{2002MNRAS.329..629S,2014arXiv1401.4171M,2013MNRAS.428.1036M}.
Using 2LPT makes generation of galaxy mocks significantly faster than
the full N-body simulations, because 2LPT allows us to compute final
positions of particles only with their initial positions without any
steps in-between. The aim of our method is the same as this approach
in the sense that we want to shorten computational time to generate
those mocks. The problem of using 2LPT is that those simulations cannot
capture non-linear gravitational evolution at the level of precision
required to resolve halos accurately and therefore this approach requires
to tune halo definitions (i.e., the linking length for the Friends-of-Friends
(FOF) algorithm) and halo masses. Also, it is hard to resolve small
halos in this approach, and therefore galaxies which reside in those
small halos are placed on randomly selected dark matter particles
in the mocks. It is, however, important to keep relatively high mass
resolution to understand the systematics caused by non-linear gravitational
evolution.

One of the simplest adjustments to an N-body simulation is to adjust
the length of each individual time step. This is the approach taken
in \cite{2014MNRAS.437.2594W,2013JCAP...06..036T}; indeed, the 2LPT
mock simulations of \cite{2013MNRAS.428.1036M,2014arXiv1401.4171M}
are an extreme version of this. These works have focused on relatively
small numbers of time steps, accurately capturing the large scale
density field, but losing information on small scales. While such
approaches are invaluable for generating the large numbers of simulations
required to build sample covariance matrices, it is harder to quantify
the impact of the loss of accuracy on systematic tests {[}\textcolor{red}{rephrase
somewhat}{]}. Our N-body simulations are consist of two components:
a long time step for solving the PM force and a set of short range
sub-cycle steps for a direct particle-particle interaction. The idea
behind is reducing the number of both time steps as much as we can
preserve enough mass resolution to correctly describe the large scale
distribution of galaxies. Our first goal in this paper is to work
at a different end of the spectrum : we aim to accurately reproduce
the details of the halo density field on large (k < ???, r>???) scales,
sacrificing the small scale structure with large time steps. Our second
goal, is to build a suite of simulations and related mock catalogs
suitable for the Baryon Oscillation Spectroscopic Survey galaxy samples.
These both serve as a demonstration of our approach, but will be useful
for evaluating systematic errors in these surveys.

In the following, we first briefly describe the mechanism of our approximated
N-body simulations. In Section 3, we test and compare our method to
the full N-body simulation and explain how we calibrate our samples.
In Section 4 and 5, we build a light cone output and populate halos
in our samples with galaxies and compute correlation functions based
on BOSS Data Release 11, and compare it with the observed correlation
function.

All simulations and calculations in this paper assume a $\Lambda$CDM
cosmology with \textcolor{red}{parameters XXX}.


\section{HACC}

The HACC framework arose from the early realization that development
of a future large-scale computational framework must be fully cognizant
of coming changes in computational architectures. It is descended
from an approach developed for the heterogeneous architecture of Roadrunner~\cite{mc3,pope10},
the first computer to break the petaflop barrier. HACC is designed
with great flexibility in mind (combining MPI with a variety of more
local programming models, e.g., OpenCL, OpenMP) and is easily adaptable
to different platforms. Currently, it is implemented on Cell-accelerated
hardware, conventional and GPU-enhanced clusters (via OpenCL), on
the IBM Blue Gene architecture (using OpenMP), and is running on prototype
Intel MIC/Xeon Phi hardware. HACC is the first, and currently the
only large-scale cosmology code suite world-wide, that can run at
scale (and beyond) on all available supercomputer architectures. Figure~\ref{hacc_scale}
shows the scaling on the entire IBM BG/Q Sequoia up to 1,572,864 cores
with an equal number of MPI ranks, attaining 13.94 PFlops at 69.2\%
of peak and 90\% parallel efficiency -- this is the highest performance
yet attained by a science code on any computer; for details, see Ref.~\cite{habib12}.
Examples of science results obtained using HACC include 64-billion
particle runs for baryon acoustic oscillations predictions for the
BOSS Lyman-$\alpha$ forest~\cite{boss_bao} and high-statistics
predictions for the halo profiles of massive clusters~\cite{conc_hacc}.
Currently, a very large 1.1 trillion particle simulation is being
carried out on the BG/Q system Mira at Argonne. A nested snapshot
at an early output time ($z\sim3$) from this simulation is shown
in Fig.~\ref{simpic}.

%FIXME --- Make sure correctly set up
\begin{figure}
\centering \includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/zoom_in}
\caption{HACC result: Zoom-in visualization of the density field in a 1.1 trillion
particle, 4.2~Gpc box size simulation with HACC on 32 BG/Q racks.
This figure illustrates the global dynamic range covered by the simulation,
$\sim10^{6}$, although the finer details are not resolved by the
visualization.}


\label{simpic} 
\end{figure}


HACC uses a hybrid parallel algorithmic structure, splitting the force
calculation into a specially designed grid-based long/medium range
spectral particle-mesh (PM) component that is common to all architectures,
and an architecture-specific short-range solver. Modular code design
combined with particle caching allows the short-range solvers to be
`hot-swappable' on-node; they are blind to the parallel implementation
of the long-range solver. The short-range solvers can use direct particle-particle
interactions, i.e., a P$^{3}$M algorithm~\cite{hockney}, as on
Roadrunner or Titan, or use tree methods as on the IBM BG/Q and Cray
XE6 systems (TreePM algorithm). The availability of multiple algorithms
within the HACC framework allows us to carry out careful error analyses,
for example, the P$^{3}$M and the TreePM versions agree to within
$0.1\%$ for the nonlinear power spectrum test in the code comparison
suite of Ref.~\cite{heitmann05}.

The HACC philosophy is that, as a general rule, both particle and
grid methods are difficult to implement as global solutions: For physics
and implementation reasons, grid-based techniques are better suited
to larger length scales, with particle methods having the opposite
property. This suggests that the higher `top' levels of code organization
should be grid-based, interacting with particle information at a lower
level of the computational hierarchy. The design ideas underlying
HACC give us a very important head start in tackling the challenges
posed by future architectures.

%FIXME --- Make sure correctly set up
\begin{figure}
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/overload}
\caption{ Overloading in a 2-D domain decomposition: A node holds particles
within its domain (green) and particle copies out to a given distance
from its boundary (red, in blue square). Copies held by neighboring
nodes form a mirrored particle cache.}


\label{overload} 
\end{figure}


\noindent On heterogeneous systems such as Roadrunner, or Titan at
Oak Ridge, HACC has the following layout: At the top level, the medium/long
range force is handled by a special low-noise, high accuracy, FFT-based
method that operates at the CPU layer. Depending on the memory balance
between the CPU and the accelerator (Cell or GPU), we can choose to
specify two different modes, (i) grids held on the CPU and particles
on the accelerator, or (ii) a streaming paradigm with grid and particle
information primarily resident in CPU memory with computations streamed
through the accelerator. In both cases, the local force solve is a
direct particle-particle interaction, i.e., the whole is a P$^{3}$M
code~\cite{hockney} with hardware acceleration, albeit an unconventional
implementation based on high-order spectral methods. The acceleration
leads to a speed-up of 50-100 times over conventional processors.
For a many-core system, the top layer of the code remains the same,
but the short-range solver changes to a tree-based algorithm which
is much better suited to the Blue Gene and Intel MIC architectures.

Our strategy simultaneously overcomes two problems: (i) how to switch
short-range solvers easily, and (ii) overcome the communication bottleneck
between the CPU and accelerator layers, or the analog for a many-core
system (on-node vs. off-node communication). We drastically reduce
the particle communication by using a mirrored particle cache, termed
`particle overloading', implemented on top of our 3-D domain decomposition
(see Fig.~\ref{overload}). The point of overloading -- loosely analogous
to ghost regions in mesh-based codes -- is to allow exact medium/long-range
force calculations with no communication of particle information and
high-accuracy local force calculations with relatively sparse refreshes
of the overloading zone (for details, see Ref.~\cite{mc3}). The
second advantage of overloading is that it frees the local force solver
from handling communication tasks, which are taken care of by the
long/medium-range force framework. Thus new `on-node' local methods
can be plugged in with no extra work and guaranteed scalability.

Conventional particle-mesh codes use a mixture of spatial filtering/differencing
and spectral techniques; the resulting complicated communication templates
can be problematic, particularly at the accelerator or many-core processor
level. To combat this, HACC uses digital filtering and differencing
in the spectral domain, allowing a simple spatial deposition technique,
Cloud-In-Cell (CIC) to be used. Combined with particle overloading,
this can reduce all communication to a negligible overhead, except
that in the parallel FFT, which lives only at the top level. A new
2-D domain decomposed (pencils instead of slabs) FFT has been implemented
to guarantee good scaling properties. The code design leads to ideal
scaling properties of HACC up to very large number of processors,
with exascale requirements already designed in to the algorithms.
Full-machine scaling has been demonstrated on Roadrunner's Open Science
phase, when MC$^{3}$ (the HACC precursor) was the only code to run
across the full machine, on Mira and Sequoia (Cf. Fig.~\ref{hacc_scale}),
and on Titan.

HACC has been ported to all the DOE Leadership class platforms; the
choice of the local force algorithm strongly depends on the architecture;
for an accelerated system (local) $N^{2}$ methods are favorable since
`compute is free' but complicated data structures are to be avoided,
while for CPU-based systems, tree algorithms are a superior choice.
HACC currently supports three production variants: (i) the Cell-accelerated
P$^{3}$M code on Cerrillos (a smaller, open version of Roadrunner);
(ii) a GPU-accelerated P$^{3}$M version with an OpenCL implementation
on OLCF's Titan; (iii) a TreePM version for many-core machines, e.g.,
ANL's Blue Gene/Q, Mira and NERSC's Hopper.

An important feature of the work proposed here is the ability to carry
out error-controlled approximate simulations at high throughput. In
order to understand how we implement this, some details of the HACC
time-stepping algorithm are now provided. HACC uses a symplectic scheme
with sub-cycling of the short-range force. The basic idea is not to
finite-difference the equations of motion, but to view evolution as
a symplectic map on phase space, written in the exponentiated form
familiar from Lie methods: $\zeta(t)=\exp(-t{\bf {H}})\zeta(0)$ where,
$\zeta$ is a phase-space vector $({\bf x},{\bf v})$, $H$ is the
Hamiltonian, and the operator, ${\bf {H}}=[H,~]_{P}$, denotes the
action of taking the Poisson bracket with the Hamiltonian. Suppose
that the Hamiltonian can be written as the sum of two parts; then
by using the Campbell-Baker-Hausdorff (CBH) theorem we can build an
integrator for the time evolution due to that Hamiltonian. Repeated
application of the CBH formula can be used to show that 
\[
\exp(-t({\bf {H}}_{1}+{\bf {H}}_{2}))=\exp(-(t/2){\bf {H}}_{1})\exp(-t{\bf {H}}_{2})\exp(-(t/2){\bf {H}}_{1})+O(t^{3})
\]
is a second order symplectic integrator. In the basic PM application,
the Hamiltonian $H_{1}$ is the free particle (kinetic) piece while
$H_{2}$ is the one-particle effective potential; corresponding respectively
to the `stream' and `kick' maps $M_{1}=\exp(-t{\bf {H}}_{1})$ and
$M_{2}=\exp(-t{\bf {H}}_{2})$. In the stream map, the particle position
is drifted using its known velocity, which remains unchanged; in the
kick map, the velocity is updated using the force evaluation, while
the position remains unchanged. This symmetric `split-operator' step
is termed SKS (stream-kick-stream). A KSK scheme constitutes an alternative
second-order symplectic integrator.

In the presence of both short and long-range forces, we split the
Hamiltonian into two parts, $H_{1}=H_{sr}+H_{lr}$ where $H_{sr}$
contains the kinetic and particle-particle force interaction (with
an associated map $M_{sr}$), whereas, $H_{2}=H_{lr}$ is just the
long range force, corresponding to the map $M_{lr}$. Since the long
range force varies relatively slowly, we construct a single time-step
map by sub cycling $M_{sr}$: $M_{full}(t)=M_{lr}(t/2)(M_{sr}(t/n_{c}))^{n_{c}}M_{lr}(t/2),$
the total map being a usual second-order symplectic integrator. This
corresponds to a KSK step, where the S is not an exact stream step,
but has enough $M_{sr}$ steps composed together to obtain the required
accuracy. (We take care that the time-dependence in the self-consistent
potential is treated correctly.) As discussed later below, we will
use the flexibility in the sub-cycling as a way of reducing the number
of time steps such that the loss of accuracy only affects the resolution
at very small scales, which are not of interest in the current set
of simulations.


\section{Convergence Test: Selection of the minimal time steps}

In this section, we examine how reducing the number of time steps
affects on halo properties (i.e., halo mass, position, and velocity)
as well as observables such as halo mass functions and halo bias.\textcolor{black}{{}
In order to quantitatively evaluate different time-stepping schemes,
we run a set of convergence tests using smaller simulation boxes.
We scale down these volumes to $(256h^{-1}{\rm Mpc})^{3}$ with $256^{3}$
particles with the same particle mass as the simulations with $(4000h^{-1}{\rm Mpc})^{3}$
. The number of time steps were chosen as 450/5, 300/3, 300/2, 150/3,
150/2, where the first number indicates the number of long time steps
and the second number the number of short time steps for each long
time step. To evaluate different time-stepping schemes, we first compare
the properties (masses, positions, and velocities) of the individual
halos themselves by matching halos in one sample to halos in another.
Following to that, we compare statistical descriptions provided by
the mass functions and power spectra. We found that differences on
halo properties do not significantly affect on those statistical observables.
Based on a number of convergence tests, the 300/2 simulation became
the final target, sufficient to resolve positions and masses of halos
reliably. }


\subsection{Matching }

Here, we compare halo properties by matching halos in different samples
one by one. We first show our algorithm for identifying the corresponding
halos in two different samples and then compare halo mass, position,
and velocity for those matched halos. From the quantitative comparison,
we find that the samples generated from the simulations with 300 global
steps have much less scatter for the baseline of the sample of the
450/5 simulation than the samples with 150 global steps. In addition
to that, we see that the differences between sub-cycles are almost
negligible.


\subsubsection{Algorithm}

Since our simulations all start with the same initial conditions,
we match halos in different simulations by matching their particle
content. Given a halo in simulation A, we consider the halos in simulation
B with the corresponding particles. Given this list of possible matches,
we match to the halo with the largest number of common particles.
To avoid spurious matches, we also require that the fraction of common
particles (relative to simulation A) exceeds a threshold. As an example
to illustrate how this matching algorithm works, we use the samples
from the 300/2 simulation and the 450/5 simulation. We adopt a threshold
of 50\% as our default choice. In Figure \ref{fig:mass-content},
we demonstrate the unmatched fraction increases with increasing threshold
and decreasing halo mass. 

Since the above matching algorithm is unidirectional, multiple halos
in the sample A might be matched to a single halo in the sample B;
this happens 1 to 2\% of the time with a matching threshold of 50\%.
We refer to these as multiply-booked halos in what follows. Figure
\ref{fig:mass-scatter1} compares halo masses matching the 450/5 simulation
to the 300/2 simulation for the case of multiply-booked halos and
the rest. The top left panel shows a mass scatter for all the matched
halos between those two simulations, while the top right panel shows
a mass scatter onl\textcolor{black}{y for the non multiply-booked
halos}. The bottom panels show a mass scatter for the case of multiply-booked
halos. The bottom left panel shows a mass scatter for individual multiply-booked
halos, while we plots a summed halo mass for those corresponding halos
in the bottom right panel. As shown in the top left panel, there are
low-mass halos in the 450/5 simulation corresponding to high-mass
halos in the 300/2 simulation. The same trend is observed for the
case of multiply-booked halos, but not for the non multiply-booked
halos. Furthermore, those disagreement for halo masses between the
two simulations are resolved by adding the corresponding halo masses.
This implies that there are multiple halos in the 450/5 simulation
which are merged into one halo in the 300/2 simulation.

Figure \ref{fig:mass-content} shows the number densities of the unmatched
halos in the 450/5 simulation matching to the 300/2 simulation at
$z=0.15$. There are three reasons that halos are considered as unmatched.
First, if particles forming a halo in the sample A do not form a halo
in the sample B (i.e., halos in the samples do not share common particles),
we consider them as unmatched. Second, if the fraction of common particles
over the total number of particles in each halo is less than 50\%,
we eliminate halos for the case of spurious matching. At last for
the case of multiply-booked halos, we remove all but the one with
the largest number of common particles. We showed each unmatched number
density as a function of halo mass. We only find unmatched halos on
low-mass regions for the reason that the halos do not have any common
particles. This is because there are some low-mass halos which are
identified in one sample but not in another sample due to the way
the FOF algorithm define halos. As shown, most of unmatched halos
are due to the threshold criterion. \textcolor{black}{We also checked
how the number of matched halos is changed as a function of redshift,
and we observed that redshift does not affect to the matching algorithm.}

\begin{figure*}
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/scatterMass_256_300_2_z0\lyxdot 15}
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/testDB_nonDB_300_2_z0\lyxdot 15}

\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/testDB_DB_numPartCut_300_2_z0\lyxdot 15}
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/testDB_sum_mass_f0\lyxdot 5_300_2_z0\lyxdot 15}

\caption{\label{fig:mass-scatter1}Comparison of halo masses matching the 450/5
simulation (x-axis) to the 300/2 simulation (y-axis) at $z=0.15$.
Panels correspond to halos with different matching criteria: all the
matched halos (top left), matched halos having one-to-one correspondence
(top right), matched halos not having one-to-one correspondence called
``multiply-booked'' halos (bottom left), and the ``multiply-booked''
halos whose corresponding halo masses are added (bottom right). Those
panels imply that large mass difference between the 450/5 simulation
and the 300/2 simulation shown in the top left panel is mainly because
those ``multiply-booked'' halos in the 450/5 simulation are merged
into one halo in the 300/2 simulation due to larger time steps.}
\end{figure*}


\begin{figure}
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/unmatchHalo2_content_300_2_z0\lyxdot 15}

\caption{\label{fig:mass-content} Itemization of unmatched halos shown as
cumulative number densities of the unmatched halos from each procedure
in the matching algorithm. The solid line is the total number density
of the unmatched halos. The green line shows halos with no counterpart
- none of the particles were identified as belonging to any halo.
As one might expect, this is only significant at low masses (\textcolor{red}{below
XXX particles}). The red line shows halos eliminated because of the
matching threshold (i.e., the halos do not have enough fraction of
the same particles). The cyan line is for the halos eliminated because
multiple halos correspond to one halo. {*}\textcolor{red}{will add
true mass function and for the case \& f=0.25 and 0.75 (which is a
fraction of matched particles in halos to consider as ``matched''
halos)}}
\end{figure}



\subsubsection{Halo Properties}

Here, we compare halo properties (i.e., halo mass, position, and velocity)
for halos matched to those in the 450/5 simulation. Since we are interested
in correctly describing the large-scale distribution of galaxies and
it requires to correctly locate dark matter halos in the simulation
with correct estimation of halo masses, it is crucial to know how
reducing the number of time steps affects on halo properties systematically.
The comparison of halo mass for different time-stepping schemes to
the 450/5 simulation at $z=0.15$ is shown in Figure \ref{fig:HaloProperty_mass}.
We take all the matched halos whose masses are between $10^{12.5}{\rm M_{\odot}}$
to $10^{13.0}{\rm M_{\odot}}$, $10^{13.0}{\rm M_{\odot}}$ to $10^{13.5}{\rm M_{\odot}}$,
and $10^{13.5}{\rm M_{\odot}}$ to $10^{14.0}{\rm M_{\odot}}$, and
compute their means and the standard deviations for ${\rm log_{10}(M/M_{450/5})}$,
where $M_{450/5}$ is a halo mass for the 450/5 simulation and $M$
corresponds to a halo in the samples generated with different time-stepping
schemes. Figure \ref{fig:HaloProperty_mass} shows that halos generated
from the simulations with small number of time steps have systematically
lower masses than those in the 450/5 simulation. This is partly because
we use the same linking length for the FOF algorithm to define halos
for all the simulations. Since reducing the number of time steps results
in an approximation to the true dynamics of dark matter fields, halos
in the 450/5 simulation tend to have tighter and denser structure
(\textcolor{red}{show the snapshot?}). So, using the same linking
length will miss connecting some particles in the simulations with
small number of time steps and will result in smaller halos than the
corresponding halo in the 450/5 simulation. The panels in Figure \ref{fig:HaloProperty_step},
from left to right, show the comparison of halo position and velocity
for the matched halos at $z=0.15$. As shown, the 150 global steps
have more scatter in the halo positions and the means d\textcolor{black}{iffer
for} the velocity difference. For the 300 global steps, the results
are significantly improved and the center position is matched in these
cases to better than 200 kpc. As is clear from Figure \ref{fig:HaloProperty_step},
the difference between 3 and 2 sub-cycles is negligible on halo properties.
Note that we observe the same trend in halo properties discussed here
at different redshifts.

\begin{figure}[t]
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/massSlice_z0\lyxdot 15}

\caption{\label{fig:HaloProperty_mass}Comparison of halo mass for matched
halos between the 450/5 simulation and other time-stepping schemes
at $z=0.15$. We take all the matched halos whose masses are between
$10^{12.5}{\rm M_{\odot}}$ to $10^{13.0}{\rm M_{\odot}}$, $10^{13.0}{\rm M_{\odot}}$
to $10^{13.5}{\rm M_{\odot}}$, and $10^{13.5}{\rm M_{\odot}}$ to
$10^{14.0}{\rm M_{\odot}}$, and compute the mean and the standard
deviation for ${\rm log_{10}(M/M_{450/5})}$ where $M_{450/5}$ is
a halo mass for the 450/5 simulation and $M$ is for the simulations
with different number of time steps corresponding to different colors
in the plot. The x-positions of the points have been displaced to
avoid overlapping the error bars. This plot shows that halo masses
become systematically smaller for the case of small number of time
steps than those in the 450/5 simulation. }
\end{figure}


\begin{figure*}[t]
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/histogram_distance_z0\lyxdot 15}\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/histogram_velocity_z0\lyxdot 15}

\caption{\label{fig:HaloProperty_step}A comparison of the positions {[}left{]}
and velocities {[}right{]} of halos matched across simulations with
different time steps. As always, the reference simulation is 450/5
shile the colors correspond to 300/3 (blue), 300/2 (green), 150/3
(red), and 150/2 (cyan). From left to right, we compared halo position
and velocity respectively.\textcolor{blue}{{} }\textcolor{black}{The
agreement with 300 time steps is very good, with a negligible difference
from the number of sub-cycles. }\textcolor{red}{{*}Is the unit for
the velocity {[}km/s{]} or with any h's? (Ask Salman and Katrin)}}
\end{figure*}



\subsection{Mass Adjustment}

In the previous subsection, Figure \ref{fig:HaloProperty_mass} shows
that halos generated by the de-tuned simulations have systematically
lower masses than the halos in the 450/5 simulation. This suggests
the necessity of adjusting halo masses for those cases to the halo
masses in the 450/5 simulation. In the following, we describe how
we do a mass adjustment and show the resulted observables including
mass functions and power spectra.


\subsubsection{Method}

To calibrate halo masses for the simulations with the reduced number
of time steps, we first take all the matched halos between the 450/5
simulation and the de-tuned simulations and compute means for each
mass bin. The reason we only take the matched halos is because our
purpose of mass adjustment here is to correct systematic mass differences
for the halos which are theoretically identical in different samples.
After computing the means for each mass bin, we fit those means to
a functional shown below so that $M_{re}$ becomes close to the average
halo masses for the 450/5 simulation:

\begin{equation}
M_{re}=M(1.0+\alpha(M/10^{12.0}[{\rm M_{\odot}])^{\beta},}\label{eq:mass_adjust}
\end{equation}
where $M_{re}$ is a reassigned halo mass, $M$ is an original halo
mass, $\alpha$ and $\beta$ are free parameters. Corresponding $\alpha$
and $\beta$ for the simulations with different number of time steps
at $z=0.15$ are listed on Table \ref{tab:free_param1}.

\begin{table}
\begin{tabular}{|c|c|c|}
\hline 
 & $\alpha$  & $\beta$\tabularnewline
\hline 
\hline 
300/3  & 0.005  & 0.175\tabularnewline
\hline 
300/2  & 0.07  & -0.47\tabularnewline
\hline 
150/3  & 0.101  & -0.162\tabularnewline
\hline 
150/2  & 0.315  & -0.411\tabularnewline
\hline 
\end{tabular}

\caption{\label{tab:free_param1}Corresponding $\alpha$ and $\beta$ in Eq.
\ref{eq:mass_adjust} for the samples with different number of time
steps at $z=0.15$.}
\end{table}


Those free parameters $\alpha$ and $\beta$ can be described as a
function of redshift. For the case of the 300/2 simulation as an example,
we find best fit parameters shown below:

\begin{equation}
\alpha(z)=0.123z+0.052,
\end{equation}
and 
\begin{equation}
\beta(z)=-0.154z-0.447.
\end{equation}



\subsubsection{Observables}

Now, we show mass functions and power spectra for the different simulations
after applying the mass adjustment.

We first compute mass functions from outputs of different time-stepping
schemes, as shown in Figure \ref{fig:massFn_step}, where we compare
simulations with reduced number of time steps to the 450/5 simulation
at $z=0.15$. In Figure \ref{fig:massFn_step}, we show the ratio
$n(>M)/n_{450/5}(>M)$, where $n_{450/5}(>M)$ is a cumulative mass
function for the 450/5 simulation and $n(>M)$ is a cumulative mass
function for different time steps shown in different colors. We compare
the results before and after mass adjustment (left and right panels
respectively). While the mass functions from the 250/3 and 150/2 simulations
are suppressed more than10\% on all mass ranges before mass adjustment,
they are significantly improved after mass adjustment, especially
on halo masses greater than $10^{13.0}{\rm M_{\odot}}$. For the simulations
with the 300 global time steps, mass adjustment is especially effective
on small halo masses.

\begin{figure}
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/haloRatioNum256_z0\lyxdot 15}
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/haloRatioNum256_tweak_z0\lyxdot 15}

\caption{\label{fig:massFn_step} Comparison of cumulative mass functions in
different simulations taking the 450/5 simulation as a reference.
Lines, from top to bottom, correspond to the simulation with different
time steps, 300/3 (blue), 300/2 (green), 150/3 (red), and 150/2 (cyan)
respectively. The left panel shows the cumulative mass functions before
adjusting masses (as described in the text), while the right panel
is after. These plots demonstrate that a simple mass recalibration
allows one to successfully recover the mass functions, even in the
extreme case of the 150/2 simulation, which differed by more than
10\% (on all mass scales). }
\end{figure}


The next measure of interest is halo-matter cross power spectra between
halo and matter density fields, as shown in Figure \ref{fig:crossMater_step}.
Figure \ref{fig:crossMater_step} shows the ratio $P_{hm}/P_{hm,450/5}$
at $z=0.15$, where $P_{hm,450/5}$ is the cross power spectrum for
the 450/5 simulation and $P_{hm}$ is the cross power spectrum for
other time steps corresponding to different colors, as labeled in
Figure \ref{fig:crossMater_step}. We use the real-space halo density
field for the left panel and the redshift-space halo densities for
the right panel in Figure \ref{fig:crossMater_step}. For the dark
matter density field, we use the output of the 450/5 simulation for
all the halo samples. Note that the dark matter density fields are
in real-space for both cases. In this way, the ratio $P_{hm}/P_{hm,450/5}$
in real-space i\textcolor{black}{s equivalent to the ratio of halo
bias between the 450/5 simulation and the simulations with other time-steps.
To select halos, we apply the soft-mass cut method using the probability
given by }

\textcolor{black}{
\begin{equation}
<N_{halo}(M)>=\frac{1}{2}{\rm erfc}(\frac{{\rm log(M_{cut}/}M)}{\sqrt{2}\sigma}),
\end{equation}
where we set ${\rm M_{cut}=10^{13.0}[{\rm M_{\odot}]}}$ and $\sigma=0.5$.
This probability has a similar form to the halo occupation distribution
(hereafter, HOD) technique so that the probability gradually becomes
one as increasing halo mass. We use this method to avoid noise from
halos scattering across sharp boundaries on halo mass.} Note that
the errors calculated here are not due to sample variance, because
we generate 10 samples from one full sample by using the soft-mass
cut method. We see that, as we decrease the number of time steps,
the ratio of the cross power spectra increases, especially in redshift-space,
we observe large deviations from one on small scales for the 150/2
and 150/3 simulations. This is due to the overall smaller halo velocities
for those simulations, which is shown in Figure \ref{fig:HaloProperty_step}.
For the simulations with the 300 global time steps, overall agreements
with the 450/5 simulation are almost 1\% on any scales in both real-space
and redshift-space.

\begin{figure*}[p]
\includegraphics[width=0.45\columnwidth]{\lyxdot \lyxdot /Plots/crossMatter_m450_tweak_softMcut13\lyxdot 0_s0\lyxdot 5_z0\lyxdot 15}\includegraphics[width=0.45\columnwidth]{\lyxdot \lyxdot /Plots/crossMatter_tweak256_redshift_z0\lyxdot 15}

\caption{\label{fig:crossMater_step} Ratio of halo-matter cross pow\textcolor{black}{er
spectra as a function of time steps with respect to the 450/5 simulation
at $z=0.15$. We use the real-space halo density field for the left
panel and the redshift-space halo density field for the right panel,
while the dark matter density fields used here are in real-space for
both cases. The left panel shows that agreements with the 450/5 simulation
are all within 2\%. In the right panel, the large discrepancy of the
cross power spectra for the simulations with 150 global steps on small
scales is mainly due to the systematically small velocities, as shown
in Figure \ref{fig:HaloProperty_step}. Note that the halos are selected
based on the soft mass-cut method with $M_{cut}=13.0$ and $\sigma=0.5$.}}
\end{figure*}


As a conclusion through several convergence tests shown in this section,
we find that the observables such as mass functions and power spectra
are not affected by the differences on halo properties as much as
those systematic differe\textcolor{black}{nces. Based on the results
in this section, we concluded that the 300/2 simulation gives sufficient
resolution for halos. }


\section{Constructing Light Cones}

We build a light cone from snapshots whose redshift range is from
$z=0.8$ to $z=0.15$ separated constantly in redshift by $\Delta z=0.1$.
The light cone is constructed with the spherical shells. Each shell
is centered at the redshift of each snapshot and has a redshift width
of 0.1. In each shell, we move halo positions by using peculiar velocities
to shift them into their light cone positions:
\begin{equation}
\vec{x}|_{z=z_{pos}}=\vec{x}|_{z=z_{snap}}+\vec{v}_{pec}|_{z=z_{snap}}\Delta t,\label{eq:lightcone}
\end{equation}
where $z_{snap}$ is the redshift of the snapshot, $z_{pos}$ is the
redshift corresponding to its radial position, $\vec{v}_{pec}$ is
its peculiar velocity, and $\Delta t$ is the time elapsed between
$z_{snap}$ and $z_{pos}$. For the case of a halo crossing its boundary
of the shell, we choose the halo whose distance from the boundary
is closer before shifting.

To evaluate how shifting affects a spatial distribution of halos,
we compare the distances for the halos at different redshift before
and after shifting their positions. For the comparison, we use the
halos which exist at both redshifts by matching halo particle profiles,
which is the same method described in Section 3.1. Figure \ref{fig:Histograms-of-distance}
is the histograms of distances for the matched halos at $z=0.25$
and $z=0.15$ before and after shifting. After shifting halos, we
see that the mean distance between corresponding halos becomes closer
and the scatter decreases. This result holds at all redshifts examined. 

Additionally, we compare the change in halo bias due to shifting.
Figure \ref{fig:power-lightcone} shows the ratios of halo matter
cross power spectra over an auto matter power spectrum at $z=0.15$.
The plot shows that shifting halos from $z=0.15$ to $z=0.25$ brings
down the halo bias to the one at $z=0.25$. The agreement between
the original power spectrum at $z=0.25$ and the shifted power spectrum
is 99.8\% up to 1$h{\rm Mpc^{-1}}$. Figure \ref{fig:power-lightcone}
shows that spatial distribution of the halos is shifted to the expected
distribution statistically. 

A parameter when building light cones by stitching static snapshots
together is how large $\Delta z$ can be between various snapshots.
Figure \ref{fig:power-lightcone2} test this in both real and redshift
space; in redshift space, we assume the velocity of the object is
unchanged between the snapshots. \textcolor{red}{We see that the agreement
in real space is within 1\% even for the case of shifting for $\Delta z=0.25$,
while the discrepancy increases more rapidly for larger $\Delta z$
in redshift space.}

\begin{figure}
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/hist_dist_z0\lyxdot 25to0\lyxdot 15}

\caption{The histogram shown in blue is that of the distances between the position
of the halos at $z=0.25$ and where they have moved to in the simulation
at $z=0.15$. The histogram shown in green is that of the distances
between the positions of the halos at $z=0.15,$ and the positions
that would be predicted by shifting the positions from the $z=0.25$
simulation assuming the peculiar velocity was constant between the
snapshots (described in Eq. \ref{eq:lightcone}). The plot shows that
the distances between the halos at different redshifts become smaller
after the shifting. \label{fig:Histograms-of-distance}}
\end{figure}


\begin{figure}
\includegraphics[width=0.5\columnwidth]{/Users/old_ts485/Dropbox/big-sims/Plots/crossPower_shift_ratio}

\caption{\label{fig:power-lightcone}In this figure, we use the halo matter
cross power spectra at $z=0.25$ and $z=0.7$ as a denominator. The
dashed lines show the ratio with the cross power spectra at $z=0.15$
and $z=0.8$ respectively before shifting, and the solid lines are
the ratio after shifting halo positions. It indicates that shifting
the positions of halos from one redshift to the another preserves
the distribution of halos statistically.}
\end{figure}


\begin{figure}
\includegraphics[width=0.45\columnwidth]{/Users/old_ts485/Dropbox/big-sims/Plots/shifting2}\includegraphics[width=0.45\columnwidth]{/Users/old_ts485/Dropbox/big-sims/Plots/shifting_s}

\caption{\label{fig:power-lightcone2}Ratio of halo matter cross power spectra
in real space {[}left{]} and in redshift space {[}right{]} after shifting
the position of the halos from the redshift labeled in the plots to
$z=0.15$. The denominator is the cross power spectra at $z=0.15,$where
we use the halo density field in real space and redshift space respectively.
For all cases, we use the DM density field in real space at $z=0.15$.
\textcolor{red}{comment for the plots? {*}will make the y-axis labels
consistent for real and redshift spaces \& make $\Delta z$ consistent
in both plots}}
\end{figure}



\section{THE BOSS SIMULATIONS}

As a concrete implementation of the approach discussed above, we construct
catalogs designed to mock the Baryon Oscillation Spectroscopic Survey
(BOSS) galaxy samples. BOSS (\cite{2013AJ....145...10D}\textcolor{red}{Is
this the one?}), part of the SDSS-III project (\cite{2011AJ....142...72E}\textcolor{red}{Is
this the one?}), is a spectroscopic survey that aims to make percent
level distance measurements using the baryon acoustic oscillation
technique. The low redshift (z < 0.7) distance measurements use two
galaxy samples : the LOWZ (z < 0.45) and CMASS (z < 0.7) samples (\cite{2014MNRAS.441...24A,2014MNRAS.440.2222T}\textcolor{red}{Are
those the ones?}). We focus on the CMASS sample below; however, the
same halo catalogs are useful for the LOWZ sample as well.

We choose a simulation volume large enough to build a full-sky mock
BOSS catalog. Since the CMASS sample extends to $z\sim0.7$, we choose
a simulation side of $4000h^{-1}{\rm Mpc}$, corresponding to a comoving
distance to $z\sim0.8$ from the center of the box; Fig. ???? shows
the BOSS CMASS redshift distribution as a function of redshift and
comoving distance (assuming our fiducial cosmology). Our simulations
are run with $4000^{3}$ particles, corresponding to a particle mass
of \textcolor{red}{?????}. The characteristic halo mass for BOSS galaxies
corresponds to $10^{13}M_{\odot}$, which we resolve with \textcolor{red}{????}
particles. We keep all halos down to \textcolor{red}{???} particles
corresponding to a halo mass of \textcolor{red}{????}. The simulations
are started at \textcolor{red}{\$z=????\$} with Zel'dovich initial
conditions and are stopped at $z=0.15$ with simulation outputs every
\textcolor{red}{\$\textbackslash{}Delta z=???\$}. At each of these
steps, we store \textcolor{red}{??????}.

The BOSS angular geometry is split into two regions : one in the North
Galactic Cap and one in the South Galactic Cap (Fig.???). Since we
generate full-sky mocks, it is straightforward to embed two full non-overlapping
BOSS surveys in a single mock realization (Fig \ref{fig:boss-geom}).
We cut out a first BOSS volume with $\vec{x}_{old}$ and then define
a new coordinate system $\vec{x}_{new}$ such as $\vec{x}_{new}=R\vec{x}_{old}$,
where $R$ is the Euler rotation matrix: 
\[
R=\left(\begin{array}{ccc}
\cos\alpha{\rm cos}\gamma-{\rm sin}\alpha{\rm cos}\beta{\rm sin}\gamma & -{\rm sin\alpha{\rm cos\gamma}-{\rm cos\alpha{\rm cos\beta}{\rm sin}\gamma}} & {\rm sin\beta sin\gamma}\\
{\rm {\rm sin}\alpha cos\beta}{\rm cos\gamma}+{\rm cos\alpha{\rm sin\gamma}} & {\rm cos}\alpha{\rm cos}\beta{\rm cos\gamma-\sin\alpha}\sin\gamma & -{\rm sin\beta cos\gamma}\\
{\rm sin\alpha{\rm sin\beta}} & {\rm cos\alpha\sin\beta} & {\rm cos\beta}
\end{array}\right),
\]
where $\alpha$, $\beta$, $\gamma$ are the Euler angles which we
rotate the Cartesian coordinates by the angles $\alpha$, $\beta$,
$\gamma$ about the z-, x-, z-axis respectively. We use $\alpha=1.35717$,
$\beta=1.67761$, $\gamma=1.64619$ for Fig \ref{fig:boss-geom}.

\begin{figure}
\includegraphics[width=0.5\columnwidth]{/Users/old_ts485/Dropbox/big-sims/Plots/boss_geom}

\caption{\label{fig:boss-geom}Demonstrating how we fit two non-overlapping
BOSS volumes into the same simulation box. The blue region is the
BOSS survey footprint in equatorial coordinates, while the red region
is the same region rotated using the rotation matrix given in the
text.}


\end{figure}



\subsection{Building the Galaxy Catalog}

To generate galaxy mock catalogs, we need to go through the following
steps:

1) populate halos with galaxies through the HOD functional form,

2) give positions and velocities to the galaxies assuming an NFW profile
\cite{1996ApJ...462..563N}.

The HOD functional form gives probabilities for the number of central
and satellite galaxies based on mass of halos which host those galaxies
with five free parameters. A halo hosts a central galaxy with probability
$N_{cen}(M)$ and a number of satellite galaxies given by a Poisson
distribution with mean $N_{sat}(M)$:

\begin{equation}
N_{cen}(M)=\frac{1}{2}{\rm erfc}\left[\frac{{\rm ln}(M_{cut}/M)}{\sqrt{2}\sigma}\right],\label{eq:Ncen}
\end{equation}
and 
\begin{equation}
N_{sat}(M)=N_{cen}(M)\left(\frac{M-\kappa M_{cut}}{M_{1}}\right)^{\alpha},\label{eq:Nsat}
\end{equation}
where $M_{cut}$, $M_{1}$, $\sigma$, $\kappa$, and $\alpha$ are
free parameters and $M$ is the halo mass. We assume that $N_{sat}(M)$
is zero when $M<\kappa M_{cut}$ and halos do not host satellite galaxies
without a central galaxy (\cite{2005ApJ...633..791Z}). The total
number of galaxies hosted by each halo is a sum of the number of central
and satellite galaxies. Eqs \ref{eq:Ncen} and \ref{eq:Nsat} are
not the only possible functional form for the HOD, and it is trivial
to change this. However, these forms reproduce the clustering of the
BOSS galaxies (\cite{2011ApJ...728..126W} \textcolor{red}{and others?})
and we use them in what follows. 

We place the central galaxies at the center, with a velocity equal
to the halo velocity. We distribute satellite galaxies with an spherically
symmetric NFW profile: 
\begin{equation}
\rho(r)=\frac{4\rho_{s}}{\frac{cr}{R_{vir}}(1+\frac{cr}{R_{vir}})^{2}},
\end{equation}
where $\rho_{s}$ is the density at the characteristic scale $r_{s}=R_{vir}/c$
, $R_{vir}$ is the virial radius for the halo and $c$ determines
how centrally concentrated the profile is. There are several measurements
of the dependence of the concentration parameter $c$ on mass and
redshift (\textcolor{red}{citations}). We adopt the form in \cite{2011ApJ...740..102K}:
\begin{equation}
c(M,z)=\frac{c_{0}}{1+z}(M-M_{0})^{-\beta},
\end{equation}
where $c_{0}=9.60$, $M_{0}=10^{12}$, and $\beta=0.75$.

The velocities of the satellite galaxies are the sum of their host
halo velocity and a random virial component. This random component
are given as follows:

\begin{equation}
<v_{x}^{2}>=<v_{y}^{2}>=<v_{z}^{2}>=\frac{1}{3}\frac{GM}{R_{vir}}.\label{eq:variance}
\end{equation}
We draw a Gaussian distribution with zero mean and variance in Eq.
\ref{eq:variance} to give each component of an internal velocity
for the satellite galaxies. Here, we assume that satellite galaxies
are randomly moving inside the host halos. So, we give the radial
component of this random motion from the Gaussian distribution and
determine the direction randomly with equal probability. 

We generate galaxy mocks from the static snapshots at $z=0.55$. In
Figure \ref{fig:xis}, we compare the correlations function $\xi(s)$
with the one in \textcolor{black}{\cite{2013arXiv1303.4666A}, where
$s$ is the redshift-space coordinate. Here, we use the following
HOD parameters, $M_{cut}=12.9$, $M_{1}=14.05$,} $\alpha=0.9$,\textcolor{black}{{}
$\kappa=1.13$, $\sigma=0.74$. }

Note that we fit the average number density of galaxies to that of
Anderson et al. 2013 as shown in \textcolor{red}{Figure XX}. In order
to fit the number density, we randomly select galaxies after populating
halos through HOD.

\textcolor{black}{The correlation function computed from our sample
mocks fits to the correlation function in Anderson et al. 2013 well
on the scale between $30h^{-1}{\rm Mpc}$ and $80h^{-1}{\rm Mpc}$.
Due to the large smoothing scale (i.e., the grid size used to compute
$\xi(s)$ is about $8h^{-1}{\rm Mpc}$), our BAO peak is more smoothed
out than the one in Anderson et al. 2013.}

{*}need to put an explanation how I compute $R_{vir}$.

{*}haven't made a plot for n(z), which is Figure XX in the text.

{*}will compute $\chi^{2}$ for $\xi$.

\begin{figure}
\includegraphics[width=0.5\columnwidth]{\lyxdot \lyxdot /Plots/xis_obs_wError}

\caption{\label{fig:xis}Correlation function monopoles $\xi(s)$ of the mocks
(green) and of the CMASS galaxy measured in \textcolor{red}{\citet{2013arXiv1303.4666A}}
(blue) at $z=0.55$. The BAO peak of the mock correlation function
is smoothed due to the wide window function. {*}Need to recompute
the correlation function with a pair counting code (right now, the
correlation function shown in the plot is computed with the periodic
boundary condition).}
\end{figure}



\section{Discussion and Summary}

In this paper, we study how reducing the number of time steps in the
N-body simulations affects halo properties and statistical observables
computed from the mocks generated by those approximated N-body simulations.
Through several convergence tests described in Section 3, we find
that systematic differences on halo properties due to the approximated
gravitational dynamics give little effects on statistical observables.
Especially, the agreement between the full N-body simulation (i.e.,
the 450/5 simulation) and the approximated simulations on halo bias
is within 1\textasciitilde{}2\% after the mass adjustment. In Section
4, we generate galaxy mocks from the sample of the 300/2 simulation
at $z=0.55$ in order to compare our result with the correlation function
appeared in \cite{2013arXiv1303.4666A}. The overall agreement on
the scale between $30h^{-1}{\rm Mpc}$ and $80h^{-1}{\rm Mpc}$ is
well within the error in the measurement.

From the results we show in this paper, we conclude that the approximated
N-body simulations can capture necessary physics to the current and
future galaxy spectroscopic surveys and will be effective enough to
achieve the precision required to those surveys.


\section*{Acknowledgement}

\bibliographystyle{plainnat}
\bibliography{bigsims}

\end{document}
